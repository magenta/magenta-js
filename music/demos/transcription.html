<!-- Copyright 2018 Google Inc. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1, user-scalable=yes">
  <link rel="stylesheet" href="./style.css">
  <title>Transcription</title>
  <style>
    input[type="file"] {
      width: 0;
      height: 0;
      opacity: 0;
      cursor: pointer;
      display: none;
    }
  </style>
</head>
<body>
  <h1>Onsets and Frames</h1>
  <p><code><a href="https://g.co/magenta/onsets-frames">Onsets and Frames</a></code>
    is a deep neural network that transcribes audio of piano performances to
    <code>NoteSequences</code>/<code>MIDI</code>.
  </p>

  <h2>From your audio file</h2>
  <p>Mention what kind of audio here</p>
  <section>
    <label class="button">
      Load audio file
      <input type="file" id="fileInput">
    </label>
    <p><b>Actual Transcription:</b> <code id="file-results" class="afileudio"></code></p>
    <p><b>It Took:</b> <code id="file-time" class="file"></code></p>
    <p><b>Total Leaked Memory:</b> <code id="file-leaked-memory"></code></b></p>
  </section>

  <h2>From Audio (250 frames / 8 seconds)</h2>
  <p>First we verify the model can transcribe a short sequence of piano audio, first computing its
    <a href="https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html" target="_blank"></a>mel spectogram</a>.
  </p>
  <section>
    <p><b>Original Audio</b><br><audio controls><source type="audio/wav" src="https://storage.googleapis.com/magentadata/js/checkpoints/transcription/onsets_frames_htk0/MAPS_MUS-mz_331_3_ENSTDkCl-250frames.wav"></audio></p>
    <p><b>Expected Transcription:</b> <code id="expected-audio-ns"></code></p>

    <button id="audioBtn">Start Audio Transcription</button>
    <p><b>Actual Transcription:</b> <code id="audio-results" class="audio"></code></p>
    <p><b>Match:</b> <code id="audio-match" class="audio"></code></p>
    <p><b>It Took:</b> <code id="audio-time" class="audio"></code></p>
    <p><b>Total Leaked Memory:</b> <code id="audio-leaked-memory"></code></b></p>
  </section>


  <h2>From <a href="https://librosa.github.io/librosa/generated/librosa.feature.melspectrogram.html" target="_blank">Mel Spectrogram</a> (250 frames / 8 seconds)</h2>
  <p>Below we verify that the model properly transcribes a mel spectrogram to match the output
    of the Python TensorFlow implementation. We found it is necessary to process the convolution
    in batches for longer inputs to avoid a GPU timeout enforced in Chrome, and we verify that
    the transcription works properly with different batch sizes that exercise various edge cases.
  </p>
  <section>
    <p><b>Original Audio</b><br><audio controls><source type="audio/wav" src="https://storage.googleapis.com/magentadata/js/checkpoints/transcription/onsets_frames_htk0/MAPS_MUS-mz_331_3_ENSTDkCl-250frames.wav"></audio></p>
    <p><b>Expected Transcription:</b> <code id="expected-ns"></code></p>
    <button id="melBtn">Start Batched Transcription</button>
  </section>

  <h3>Batch Length 250 / Batch Size 1</h3>
  <section>
    <p><b>Actual Transcription:</b> <code id="250-results" class="batch"></code></p>
    <p><b>Match:</b> <code id="250-match" class="batch"></code></p>
    <p><b>It Took:</b> <code id="250-time" class="batch"></code></p>
  </section>

  <h3>Batch Length 150 / Batch Size 2</h3>
  <section>
    <p><b>Actual Transcription:</b> <code id="150-results" class="batch"></code></p>
    <p><b>Match:</b> <code id="150-match" class="batch"></code></p>
    <p><b>It Took:</b> <code id="150-time" class="batch"></code></p>
  </section>

  <h3>Batch Length 80 / Batch Size 4</h3>
  <section>
    <p><b>Actual Transcription:</b> <code id="80-results" class="batch"></code></p>
    <p><b>Match:</b> <code id="80-match" class="batch"></code></p>
    <p><b>It Took:</b> <code id="80-time" class="batch"></code></p>
  </section>

  <h3>Batch Length 62 / Batch Size 4</h3>
  <section>
    <p><b>Actual Transcription:</b> <code id="62-results" class="batch"></code></p>
    <p><b>Match:</b> <code id="62-match" class="batch"></code></p>
    <p><b>It Took:</b> <code id="62-time" class="batch"></code></p>
  </section>

  <h3>Batch Length 50 / Batch Size 5</h3>
  <section>
    <p><b>Actual Transcription:</b> <code id="50-results" class="batch"></code></p>
    <p><b>Match:</b> <code id="50-match" class="batch"></code></p>
    <p><b>It Took:</b> <code id="50-time" class="batch"></code></p>
  </section>

  <section>
    <p><b>Total Leaked Memory:</b> <code id="leaked-memory"></code></b></p>
  </section>

  <script src="transcription_bundle.js"></script>
</body>
</html>
