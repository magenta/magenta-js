<!-- Copyright 2018 Google Inc. All Rights Reserved.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================-->
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1, user-scalable=yes">
  <link rel="stylesheet" href="./style.css">
  <title>MidiME</title>
  <style>
    input[type="file"] {
      width: 0;
      height: 0;
      opacity: 0;
      cursor: pointer;
      display: none;
    }
  </style>
</head>
<body>
  <h1>MidiME</h1>
  <p><code>MidiME</code> is a variational autoencoder that lets you personalize
    your own <code>MusicVAE</code> model with just a little data, so that samples
    from <code>MidiME</code> sound closer to your provided data.</p>
  <h2>Input data</h2>
  <section>
    <p>This is what you want the samples to sound like. Note that MusicVAE is a
      monophonic model, so if you use a polyphonic MIDI file, you're not
      guaranteed to reconstruct the "main" melody.</p>
      <p>Try training on a single full song to get outputs that sound like
        variations on it, or train on multiple songs to get samples that
        combine various characteristics of them</p>
    <label class="button" id="fileBtn" disabled>
      Load midi file
      <input type="file" id="fileInput">
    </label>
    <p><code id="input"></code>
  </section>

  <h2>Training</h2>
  <p>The MidiMe model works by training a variational autoencoder in the
    browser. The more steps you train for, the better the input reconstruction will be.
  </p>
  <section>
    <svg id="graph"></svg>
  </section>

  <h2>Input reconstruction</h2>
  <section>
    <p><b>Before training:</b> <code id="pre-training"></code></p>
    <p><b>After training:</b> <code id="post-training"></code></p>
    <p><b>It took:</b> <code id="training-time"></code></p>
  </section>

  <h2>Random samples</h2>
  <p>You can now sample from MidiMe. If you trained for long enough, then
    these samples will be very similar to the input data. Contrast this with
    random samples from MusicVAE, which do not sound like the input data.
  </p>
  <p>Each of the examples below is 5 concatenated 2-bar samples.</p>
  <section>
    <p><b>From MusicVAE</b> <code id="sample-musicvae"></code></p>
    <p><b>From trained MidiMe</b> <code id="sample-midime"></code></p>
  </section>

  <section>
    <p><b>Total Leaked Memory:</b> <code id="leaked-memory"></code></b></p>
  </section>
  <script src="midi_me_bundle.js"></script>
</body>
</html>
