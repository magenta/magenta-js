# @magenta/music

This JavaScript implementation of Magenta's musical note-based models uses [TensorFlow.js](https://js.tensorflow.org) for GPU-accelerated inference.

Complete documentation is available at https://tensorflow.github.io/magenta-js.

For the Python TensorFlow implementations, see the [main Magenta repo](https://github.com/tensorflow/magenta).

## Contents

* [Example Applcations](#example-applications)
* [Supported Models](#supported-models)
* [Getting Started](#getting-started)
* [Model Checkpoints](#model-checkpoints)

## Example Applications

Here are a few applications built with MagentaMusic.js:

* [Beat Blender](https://g.co/beatblender) by [Google Creative Lab](https://github.com/googlecreativelab)
* [Melody Mixer](https://g.co/melodymixer) by [Google Creative Lab](https://github.com/googlecreativelab)
* [Latent Loops](https://goo.gl/magenta/latent-loops) by [Google Pie Shop](https://github.com/teampieshop)
* [Neural Drum Machine](https://codepen.io/teropa/pen/RMGxOQ) by [Tero Parviainen](https://github.com/teropa)

## Supported Models

We have made an effort to port our most useful models, but please file an issue if you think something is
missing, or feel free to submit a Pull Request!

### MusicRNN
[MusicRNN](./src/music_rnn) implements Magenta's LSTM-based language models. These include [MelodyRNN][melody-rnn], [DrumsRNN][drums-rnn], [ImprovRNN][improv-rnn], and [PerformanceRNN][performance-rnn].

### MusicVAE
[MusicVAE](./src/music_vae) implements several configurations of Magenta's variational autoencoder model called [MusicVAE][music-vae] including melody and drum "loop" models, 4- and 16-bar "trio" models, and chord-conditioned "multi-track" models.

## Getting started

There are two main ways to get MagentaMusic.js in your JavaScript project:
via [script tags](https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use_JavaScript_within_a_webpage) **or** by installing it from [NPM](https://www.npmjs.com/)
and using a build tool like [yarn](https://yarnpkg.com/en/).

### via Script Tag

Add the following code to an HTML file:

```html
<html>
  <head>
    <!-- Load @magenta/music -->
    <script src="https://cdn.jsdelivr.net/npm/@magenta/music@1.0.0"></script>

    <!-- Place your code in the script tag below. You can also use an external .js file -->
    <script>
      // Instantiate the model by loading the desired checkpoint.
      const model = new mm.MusicVAE(
          'https://storage.googleapis.com/download.magenta.tensorflow.org/' +
          'tfjs_checkpoints/music_vae/trio_4bar_lokl_small_q1');
      const player = new mm.Player();

      const start = () => {
        document.getElementById("start").style.display = "none";
        mm.Player.tone.context.resume();
        model.initialize().then(
          // Endlessly sample and play back the result.
          function sampleAndPlay() {
            return model.sample(1)
                .then((samples) => player.start(samples[0]))
                .then(sampleAndPlay);
          })};
    </script>
  </head>
  <body><button id="start" onclick="start()">Start</button></body>
</html>
```

Open up that html file in your browser (or [click here](https://codepen.io/adarob/pen/gzwJZL/) for a hosted version)
and the code will run. After a few seconds you'll hear an endless stream of 4-bar
trios that are randomly generated by MusicVAE!

See the [Neural Drum Machine](https://codepen.io/teropa/pen/RMGxOQ) by [@teropa](https://github.com/teropa) for a complete example application with code.

### via NPM

Add [MagentaMusic.js][mm-npm] to your project using [yarn](https://yarnpkg.com/en/) **or** [npm](https://docs.npmjs.com/cli/npm).
For example, with yarn you can simply call `yarn add @magenta/music`.

Then, you can use the library in your own code as in the following example:

```js
import * as mm from '@magenta/music';

const model = new mm.MusicVAE('/path/to/checkpoint');
const player = new mm.Player();

model.initialize()
    .then(() => model.sample(1))
    .then((samples) => player.start(samples[0]));
```

See our [demos](./demos) for example usage.

#### Example Commands

`yarn install` to install dependencies.

`yarn test` to run tests.

`yarn bundle` to produce a bundled version in `dist/`.

`yarn run-demos` to build and run the demo.

## Model Checkpoints

Since MagentaMusic.js does not support training models, you must use weights from a model trained with the Python-based [Magenta models][magenta-models]. We are also making available our own hosted pre-trained checkpoints.

### Magenta-Hosted Checkpoints

Several pre-trained MusicRNN and MusicVAE checkpoints are hosted on GCS. The full list can is available in [this table](checkpoints/README.md#table) and can be accessed programmatically via a JSON index at https://goo.gl/magenta/js-checkpoints-json.

More information is available at https://goo.gl/magenta/js-checkpoints.

### Your Own Checkpoints

#### Dumping Your Weights
To use your own checkpoints with one of our models, you must first convert the weights to the appropriate format using the provided [checkpoint_converter](../scripts/checkpoint_converter.py) script.

This tool is dependent on [tfjs-converter](https://github.com/tensorflow/tfjs-converter), which you must first install using `pip install tensorflowjs`. Once installed, you can execute the script as follows:

```bash
../scripts/checkpoint_converter.py /path/to/model.ckpt /path/to/output_dir
```

There are additonal flags available to reduce the size of the output by removing unused (training) variables or using weight quantization. Call `../scripts/checkpoint_converter.py -h` to list the avilable options.

#### Specifying the Model Configuration

The model configuration should be placed in a JSON file named `config.json` in the same directory as your checkpoint. This configuration file contains all the information needed (besides the weights) to instantiate and run your model: the model type and data converter specification plus optional chord encoding, auxiliary inputs, and attention length. An example `config.json` file might look like:

```json
{
  "type": "MusicRNN",
    "dataConverter": {
      "type": "MelodyConverter",
      "args": {
        "minPitch": 48,
        "maxPitch": 83
      }
    },
    "chordEncoder": "PitchChordEncoder"
}
```

This configuration corresponds to a chord-conditioned melody MusicRNN model.

<!-- links -->
[melody-rnn]: https://github.com/tensorflow/magenta/tree/master/magenta/models/melody_rnn
[drums-rnn]: https://github.com/tensorflow/magenta/tree/master/magenta/models/drums_rnn
[improv-rnn]: https://github.com/tensorflow/magenta/tree/master/magenta/models/improv_rnn
[performance-rnn]: https://github.com/tensorflow/magenta/tree/master/magenta/models/performance_rnn
[magenta-models]: https://github.com/tensorflow/magenta/tree/master/magenta/models
[music-vae]: https://g.co/magenta/musicvae
[mm-npm]: https://www.npmjs.com/package/@magenta/music
